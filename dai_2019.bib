@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@phdthesis{smith2007probabilistic,
  title={Probabilistic planning for robotic exploration},
  author={Smith, Trey and Simmons, Reid},
  year={2007},
  school={Carnegie Mellon University, The Robotics Institute}
}

@inproceedings{bai2015intention,
  title={Intention-aware online POMDP planning for autonomous driving in a crowd},
  author={Bai, Haoyu and Cai, Shaojun and Ye, Nan and Hsu, David and Lee, Wee Sun},
  booktitle={2015 ieee international conference on robotics and automation (icra)},
  pages={454--460},
  year={2015},
  organization={IEEE}
}

@article{mccallum1997reinforcement,
  title={Reinforcement learning with selective perception and hidden state},
  author={McCallum, R},
  year={1997}
}

@inproceedings{liu2016no,
  title={No-fringe u-tree: An optimized algorithm for reinforcement learning},
  author={Liu, Feng and Jin, Xin and She, Yunfeng},
  booktitle={2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)},
  pages={278--282},
  year={2016},
  organization={IEEE}
}

@article{li2017deep,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@article{wiering1997hq,
  title={HQ-learning},
  author={Wiering, Marco and Schmidhuber, J{\"u}rgen},
  journal={Adaptive Behavior},
  volume={6},
  number={2},
  pages={219--246},
  year={1997},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@inproceedings{bonet1998solving,
  title={Solving large POMDPs using real time dynamic programming},
  author={Bonet, Blai},
  booktitle={In Proc. AAAI Fall Symp. on POMDPs},
  year={1998},
  organization={Citeseer}
}

@article{shani2013survey,
  title={A survey of point-based POMDP solvers},
  author={Shani, Guy and Pineau, Joelle and Kaplow, Robert},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={27},
  number={1},
  pages={1--51},
  year={2013},
  publisher={Springer}
}

@incollection{mccallum1995instance,
  title={Instance-based utile distinctions for reinforcement learning with hidden state},
  author={McCallum, R Andrew},
  booktitle={Machine Learning Proceedings 1995},
  pages={387--395},
  year={1995},
  publisher={Elsevier}
}

@article{aberdeen2003policy,
  title={Policy-gradient algorithms for partially observable Markov decision processes},
  author={Aberdeen, Douglas and others},
  year={2003},
  publisher={The Australian National University}
}

@inproceedings{meuleau1999learning,
  title={Learning finite-state controllers for partially observable environments},
  author={Meuleau, Nicolas and Peshkin, Leonid and Kim, Kee-Eung and Kaelbling, Leslie Pack},
  booktitle={Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence},
  pages={427--436},
  year={1999},
  organization={Morgan Kaufmann Publishers Inc.}
}